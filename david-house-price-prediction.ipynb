{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":15643,"databundleVersionId":629503,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-07-10T13:06:08.450426Z","iopub.execute_input":"2025-07-10T13:06:08.450861Z","iopub.status.idle":"2025-07-10T13:06:08.460324Z","shell.execute_reply.started":"2025-07-10T13:06:08.450827Z","shell.execute_reply":"2025-07-10T13:06:08.459262Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.preprocessing import OneHotEncoder\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2025-07-10T13:06:08.461829Z","iopub.execute_input":"2025-07-10T13:06:08.462207Z","iopub.status.idle":"2025-07-10T13:06:08.480548Z","shell.execute_reply.started":"2025-07-10T13:06:08.462175Z","shell.execute_reply":"2025-07-10T13:06:08.479404Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load datasets\ntrain_df = pd.read_csv(\"/kaggle/input/neolen-house-price-prediction/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/neolen-house-price-prediction/test.csv\")\n\n# Drop the Id column\ntrain_df.drop(\"Id\", axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2025-07-10T13:06:08.481769Z","iopub.execute_input":"2025-07-10T13:06:08.482102Z","iopub.status.idle":"2025-07-10T13:06:08.525123Z","shell.execute_reply.started":"2025-07-10T13:06:08.482054Z","shell.execute_reply":"2025-07-10T13:06:08.524160Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Summary statistics to get overview sight of the dataset\nprint(\"Summary Statistics:\")\n'''\nGenerates summary statistics for each column of the DataFrame train_df:\n- count: The number of non-null (non-NaN) entries in the column.\n- mean: The average value of the column (only relevant for numeric data).\n- std: The standard deviation, which measures the amount of variation or dispersion in the column.\n- min: The smallest value in the column.\n- 25%: The 25th percentile, also known as the first quartile. This indicates that 25% of the data falls below this value.\n- 50% (median): The median value, where half of the data falls below this value.\n- 75%: The 75th percentile, or third quartile, indicating that 75% of the data falls below this value.\n- max: The largest value in the column.\n'''\nprint(train_df.describe(include='all'))\n","metadata":{"execution":{"iopub.status.busy":"2025-07-10T13:06:08.526723Z","iopub.execute_input":"2025-07-10T13:06:08.527073Z","iopub.status.idle":"2025-07-10T13:06:08.653137Z","shell.execute_reply.started":"2025-07-10T13:06:08.527047Z","shell.execute_reply":"2025-07-10T13:06:08.652373Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize outliers in key features\ndef plot_outliers(df, feature):\n    plt.figure(figsize=(8, 4))\n    sns.boxplot(x=df[feature])\n    plt.title(f\"Boxplot of {feature}\")\n    plt.show()\n\nfor col in [\"GrLivArea\", \"TotalBsmtSF\", \"SalePrice\"]:\n    if col in train_df.columns:\n        plot_outliers(train_df, col)\n\n# Remove outliers using IQR method for selected features\ndef remove_outliers(df, features):\n    for feature in features:\n        if feature in df.columns:\n            Q1 = df[feature].quantile(0.25)\n            Q3 = df[feature].quantile(0.75)\n            IQR = Q3 - Q1\n            lower = Q1 - 1.5 * IQR\n            upper = Q3 + 1.5 * IQR\n            df = df[(df[feature] >= lower) & (df[feature] <= upper)]\n    return df\n\n# Store original shape for comparison\noriginal_shape = train_df.shape\ntrain_df = remove_outliers(train_df, [\"GrLivArea\", \"TotalBsmtSF\", \"SalePrice\"])\nprint(f\"Removed {original_shape[0] - train_df.shape[0]} outlier rows. New shape: {train_df.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T13:06:08.654530Z","iopub.execute_input":"2025-07-10T13:06:08.654868Z","iopub.status.idle":"2025-07-10T13:06:08.996838Z","shell.execute_reply.started":"2025-07-10T13:06:08.654836Z","shell.execute_reply":"2025-07-10T13:06:08.995952Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Outlier Detection and Removal\nOutliers can negatively impact model performance. We'll visualize and remove outliers from key numerical features using the IQR method.","metadata":{}},{"cell_type":"code","source":"# Filtering out missing values columns and suming missing cells\nmissing_values = train_df.isnull().sum()\n# Sorting missing columns by missing rows\nmissing_values = missing_values[missing_values > 0].sort_values(ascending=False)\nprint(\"\\nMissing Values:\")\nprint(missing_values)","metadata":{"execution":{"iopub.status.busy":"2025-07-10T13:06:08.997831Z","iopub.execute_input":"2025-07-10T13:06:08.998068Z","iopub.status.idle":"2025-07-10T13:06:09.008992Z","shell.execute_reply.started":"2025-07-10T13:06:08.998050Z","shell.execute_reply":"2025-07-10T13:06:09.007947Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize distributions of numerical features\n\n## Filter out numeric type columns\nnumerical_cols = train_df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n## Creates histograms for each of the numerical columns selected from train_df\n## - Set width of 15 inches and a height of 12 inches are specified,\n## - Indicates that the data should be divided into 30 equally spaced bins\ntrain_df[numerical_cols].hist(figsize=(15, 12), bins=30)\nplt.suptitle(\"Distributions of Numerical Features\", fontsize=16)\n## Adjust layout to prevent overlap\nplt.tight_layout()\n## Display the histograms\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2025-07-10T13:06:09.011720Z","iopub.execute_input":"2025-07-10T13:06:09.011995Z","iopub.status.idle":"2025-07-10T13:06:15.143941Z","shell.execute_reply.started":"2025-07-10T13:06:09.011973Z","shell.execute_reply":"2025-07-10T13:06:15.143149Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize categorical features and their relationship with SalePrice\ncategorical_cols = train_df.select_dtypes(include=[\"object\"]).columns\nplt.figure(figsize=(12, 10))\nfor i, col in enumerate(categorical_cols[:8]):  # Show top 4 for brevity\n    plt.subplot(3, 3, i+1)\n    sns.boxplot(x=col, y=\"SalePrice\", data=train_df)\n    plt.xticks(rotation=45)\n    plt.title(f\"SalePrice by {col}\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T13:06:15.145328Z","iopub.execute_input":"2025-07-10T13:06:15.145640Z","iopub.status.idle":"2025-07-10T13:06:16.421226Z","shell.execute_reply.started":"2025-07-10T13:06:15.145614Z","shell.execute_reply":"2025-07-10T13:06:16.420161Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Exploratory Data Analysis: Categorical Features\nLet's visualize the distribution of some important categorical features and their relationship with SalePrice.","metadata":{}},{"cell_type":"code","source":"# Compute the correlation matrix\n# This matrix shows pairwise correlation between numerical features\ncorrelation_matrix = train_df[numerical_cols].corr()\n\n# Plot the correlation heatmap\n# This visualizes the correlation matrix using a color-coded heatmap\nplt.figure(figsize=(16, 12))\nsns.heatmap(correlation_matrix, annot=False, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\nplt.title(\"Correlation Heatmap of Numerical Features\")\nplt.show()\n\n# Print correlation values with SalePrice\n# This helps identify which features are most strongly related to the target variable\nsaleprice_correlation = correlation_matrix[\"SalePrice\"].sort_values(ascending=False)\nprint(\"Correlation of features with SalePrice:\\n\")\nprint(saleprice_correlation)\n","metadata":{"execution":{"iopub.status.busy":"2025-07-10T13:06:16.422514Z","iopub.execute_input":"2025-07-10T13:06:16.422865Z","iopub.status.idle":"2025-07-10T13:06:17.172674Z","shell.execute_reply.started":"2025-07-10T13:06:16.422843Z","shell.execute_reply":"2025-07-10T13:06:17.171734Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Drop the Id column\ntest_ids = test_df[\"Id\"]\ntest_df.drop(\"Id\", axis=1, inplace=True)\n\n# Separate features and target\nX = train_df.drop(\"SalePrice\", axis=1)\ny = train_df[\"SalePrice\"]","metadata":{"execution":{"iopub.status.busy":"2025-07-10T13:06:17.173665Z","iopub.execute_input":"2025-07-10T13:06:17.174017Z","iopub.status.idle":"2025-07-10T13:06:17.182648Z","shell.execute_reply.started":"2025-07-10T13:06:17.173995Z","shell.execute_reply":"2025-07-10T13:06:17.181640Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Identify numerical and categorical columns\nnumerical_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\ncategorical_cols = X.select_dtypes(include=[\"object\"]).columns\n\nprint(\"numerical_cols\\n\", numerical_cols)\nprint(\"categorical_cols\\n\", categorical_cols)","metadata":{"execution":{"iopub.status.busy":"2025-07-10T13:06:17.183715Z","iopub.execute_input":"2025-07-10T13:06:17.184001Z","iopub.status.idle":"2025-07-10T13:06:17.201406Z","shell.execute_reply.started":"2025-07-10T13:06:17.183971Z","shell.execute_reply":"2025-07-10T13:06:17.200445Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Log Transformation of SalePrice\nThe SalePrice variable is often right-skewed. Applying a log transformation can help normalize the distribution and improve model performance.","metadata":{}},{"cell_type":"code","source":"# Handle missing values\nnumerical_imputer = SimpleImputer(strategy=\"median\")\ncategorical_imputer = SimpleImputer(strategy=\"most_frequent\")\n\n\n# Encoding categorical variables\ncategorical_encoder = OneHotEncoder(handle_unknown=\"ignore\")\n\n# Feature scaling\nscaler = StandardScaler()\n\n# Create preprocessing pipelines\nnumerical_pipeline = Pipeline([\n    (\"imputer\", numerical_imputer),\n    (\"scaler\", scaler)\n])\n\ncategorical_pipeline = Pipeline([\n    (\"imputer\", categorical_imputer),\n    (\"encoder\", categorical_encoder)\n])\n\n# Combine preprocessing\npreprocessor = ColumnTransformer([\n    (\"num\", numerical_pipeline, numerical_cols),\n    (\"cat\", categorical_pipeline, categorical_cols)\n])","metadata":{"execution":{"iopub.status.busy":"2025-07-10T13:06:17.202455Z","iopub.execute_input":"2025-07-10T13:06:17.202754Z","iopub.status.idle":"2025-07-10T13:06:17.218778Z","shell.execute_reply.started":"2025-07-10T13:06:17.202731Z","shell.execute_reply":"2025-07-10T13:06:17.217775Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Feature selection\nfeature_selector = SelectKBest(score_func=f_regression, k=50)\n\n# Create full pipeline\nmodel_pipeline = Pipeline([\n    (\"preprocessor\", preprocessor),\n    (\"feature_selection\", feature_selector),\n    (\"regressor\", LinearRegression())\n])\n\n# Fit the model\nmodel_pipeline.fit(X, y)\n\n# Predict on test data\npredictions = model_pipeline.predict(test_df)\n\n# Save predictions to CSV\noutput_df = pd.DataFrame({\"Id\": test_ids, \"SalePrice\": predictions})\noutput_df.to_csv(\"house_price_predictions.csv\", index=False)\n","metadata":{"execution":{"iopub.status.busy":"2025-07-10T13:06:17.219698Z","iopub.execute_input":"2025-07-10T13:06:17.219963Z","iopub.status.idle":"2025-07-10T13:06:17.319413Z","shell.execute_reply.started":"2025-07-10T13:06:17.219941Z","shell.execute_reply":"2025-07-10T13:06:17.318614Z"},"trusted":true},"outputs":[],"execution_count":null}]}